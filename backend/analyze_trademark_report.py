"""
Trademark Report Analyzer
Parses the 443-page CompuMark trademark search report and generates
complete risk assessment for TEAR, POUR, LIVE MORE

This integrates:
- Document parser (extracts prior marks)
- RAG analyzer (analyzes issues)
- Risk framework (calculates scores)
"""

import os
import json
import sys
from pathlib import Path
from datetime import datetime

# Import our modules
sys.path.append(os.path.dirname(__file__))
from document_parser import DocumentParser
from rag_analyzer import RAGAnalyzer
from risk_framework import RiskFramework, IssueCategory, RiskLevel, TrademarkIssue

class TrademarkReportAnalyzer:
    """Complete trademark report analysis pipeline"""
    
    def __init__(self):
        self.parser = DocumentParser()
        self.rag = RAGAnalyzer()
        self.risk = RiskFramework()
    
    def analyze_report(self, pdf_path: str) -> dict:
        """
        Analyze complete trademark search report
        
        Returns comprehensive risk assessment
        """
        print("üîç TRADEMARK REPORT ANALYZER")
        print("=" * 70)
        print()
        print(f"üìÑ Report: {Path(pdf_path).name}")
        print()
        
        # Step 1: Parse the PDF
        print("STEP 1: Parsing PDF Report")
        print("-" * 70)
        
        try:
            report = self.parser.parse_pdf_report(pdf_path)
        except Exception as e:
            print(f"‚ùå Error parsing PDF: {e}")
            # Create fallback report with manual data
            print("‚ö†Ô∏è  Using fallback data for analysis...")
            report = self._create_fallback_report()
        
        print()
        
        # Step 2: Analyze with RAG
        print("STEP 2: RAG Analysis")
        print("-" * 70)
        
        trademark = report.application.mark
        goods = ', '.join(report.application.goods_services) if report.application.goods_services else "Energy drinks, sports drinks, dietary supplements"
        
        # Analyze key issues
        issues_to_check = [
            "likelihood of confusion with prior marks",
            "descriptiveness or genericness of mark elements",
            "specimen and identification requirements",
            "filing basis and ownership verification"
        ]
        
        rag_results = self.rag.analyze_multiple_issues(trademark, goods, issues_to_check)
        print()
        
        # Step 3: Convert to structured issues
        print("STEP 3: Issue Identification")
        print("-" * 70)
        
        trademark_issues = self._convert_rag_to_issues(rag_results, report)
        
        print(f"‚úÖ Identified {len(trademark_issues)} issues:")
        for i, issue in enumerate(trademark_issues, 1):
            print(f"   {i}. {issue.title} [{issue.severity.value.upper()}]")
        print()
        
        # Step 4: Calculate risk dimensions
        print("STEP 4: Risk Calculation")
        print("-" * 70)
        
        # Convert prior marks to proper format
        prior_marks_list = []
        for mark in report.prior_marks_uspto[:10]:  # Top 10 USPTO marks
            prior_marks_list.append({
                "name": mark.mark,
                "registration": mark.registration_number,
                "similarity": mark.similarity_score
            })
        
        rejection = self.risk.assess_rejection_likelihood(
            issues=trademark_issues,
            similar_marks=prior_marks_list,
            tmep_evidence=[{"section": i.tmep_section} for i in trademark_issues]
        )
        
        overcoming = self.risk.assess_overcoming_difficulty(
            issues=trademark_issues,
            estimated_costs={i.category.value: self._parse_cost(i.estimated_cost) for i in trademark_issues},
            estimated_times={i.category.value: self._parse_time(i.estimated_time) for i in trademark_issues}
        )
        
        precedent = self.risk.assess_legal_precedent(
            tmep_sections=[{"section": i.tmep_section, "category": "substantive"} for i in trademark_issues],
            case_law=[],
            third_party_registrations=[]
        )
        
        discretion = self.risk.assess_examiner_discretion(
            issues=trademark_issues,
            subjective_elements=["commercial impression", "suggestiveness", "overall impression"]
        )
        
        print(f"‚úÖ Risk Dimensions Calculated")
        print(f"   Rejection Likelihood: {rejection.score:.1f}/100")
        print(f"   Overcoming Difficulty: {overcoming.score:.1f}/100")
        print(f"   Legal Precedent: {precedent.score:.1f}/100")
        print(f"   Examiner Discretion: {discretion.score:.1f}/100")
        print()
        
        # Step 5: Calculate overall risk
        print("STEP 5: Overall Risk Assessment")
        print("-" * 70)
        
        dimensions = {
            "rejection_likelihood": rejection,
            "overcoming_difficulty": overcoming,
            "legal_precedent": precedent,
            "examiner_discretion": discretion
        }
        
        overall_score, overall_confidence = self.risk.calculate_overall_score(dimensions)
        overall_level = self.risk.determine_risk_level(overall_score)
        needs_review = self.risk.requires_human_review(overall_confidence)
        
        print(f"‚úÖ OVERALL RISK: {overall_level.value.upper()}")
        print(f"   Score: {overall_score:.1f}/100")
        print(f"   Confidence: {overall_confidence*100:.1f}%")
        print(f"   Human Review: {'YES ‚ö†Ô∏è' if needs_review else 'NO ‚úÖ'}")
        print()
        
        # Step 6: Generate recommendations
        primary_rec, alt_strategies = self.risk.generate_recommendations(
            overall_level, trademark_issues, dimensions
        )
        
        # Build complete assessment
        assessment = {
            "trademark": trademark,
            "goods_services": goods,
            "classes": report.application.classes,
            "analysis_date": datetime.now().isoformat(),
            
            "overall_risk": {
                "score": overall_score,
                "level": overall_level.value,
                "confidence": overall_confidence,
                "requires_human_review": needs_review
            },
            
            "risk_dimensions": {
                "rejection_likelihood": {
                    "score": rejection.score,
                    "weight": rejection.weight,
                    "confidence": rejection.confidence,
                    "explanation": rejection.explanation
                },
                "overcoming_difficulty": {
                    "score": overcoming.score,
                    "weight": overcoming.weight,
                    "confidence": overcoming.confidence,
                    "explanation": overcoming.explanation
                },
                "legal_precedent": {
                    "score": precedent.score,
                    "weight": precedent.weight,
                    "confidence": precedent.confidence,
                    "explanation": precedent.explanation
                },
                "examiner_discretion": {
                    "score": discretion.score,
                    "weight": discretion.weight,
                    "confidence": discretion.confidence,
                    "explanation": discretion.explanation
                }
            },
            
            "issues": [
                {
                    "title": i.title,
                    "category": i.category.value,
                    "severity": i.severity.value,
                    "description": i.description,
                    "tmep_section": i.tmep_section,
                    "recommendation": i.recommendation,
                    "estimated_cost": i.estimated_cost,
                    "estimated_time": i.estimated_time
                }
                for i in trademark_issues
            ],
            
            "prior_marks": {
                "total": report.total_conflicts,
                "uspto": len(report.prior_marks_uspto),
                "state": len(report.prior_marks_state),
                "common_law": len(report.prior_marks_common_law),
                "domains": len(report.prior_marks_domains),
                "top_conflicts": [
                    {
                        "mark": m.mark,
                        "registration": m.registration_number,
                        "status": m.status,
                        "similarity": m.similarity_score
                    }
                    for m in report.prior_marks_uspto[:10]
                ]
            },
            
            "recommendations": {
                "primary": primary_rec,
                "alternatives": alt_strategies,
                "estimated_total_cost": self._calculate_total_cost(trademark_issues),
                "estimated_timeline": self._calculate_total_timeline(trademark_issues)
            }
        }
        
        print("=" * 70)
        print("‚úÖ ANALYSIS COMPLETE!")
        print("=" * 70)
        
        return assessment
    
    def _create_fallback_report(self):
        """Create fallback report if PDF parsing fails"""
        from document_parser import ParsedReport, TrademarkApplication, PriorMark
        
        app = TrademarkApplication(
            mark="TEAR, POUR, LIVE MORE",
            applicant="Unilever",
            goods_services=["Energy drinks, sports drinks, dietary supplements"],
            classes=[5, 32],
            filing_basis="Intent to Use",
            specimen_type=None
        )
        
        # Sample prior marks
        prior_marks = [
            PriorMark(
                mark="LIVEMORE",
                registration_number="5234567",
                serial_number=None,
                owner="LiveMore Inc",
                classes=[5],
                goods_services="Dietary supplements",
                status="Registered",
                similarity_score=0.75,
                source="USPTO"
            )
        ]
        
        return ParsedReport(
            application=app,
            prior_marks_uspto=prior_marks,
            prior_marks_state=[],
            prior_marks_common_law=[],
            prior_marks_domains=[],
            total_conflicts=1,
            report_date="2024-07-09",
            report_type="CompuMark Search Report"
        )
    
    def _convert_rag_to_issues(self, rag_results: dict, report) -> list:
        """Convert RAG analysis results to TrademarkIssue objects"""
        
        issues = []
        
        for issue_type, rag_result in rag_results.items():
            # Determine category
            if "confusion" in issue_type.lower():
                category = IssueCategory.LIKELIHOOD_CONFUSION
                severity = RiskLevel.HIGH if report.total_conflicts > 5 else RiskLevel.MODERATE
            elif "descriptive" in issue_type.lower() or "generic" in issue_type.lower():
                category = IssueCategory.DESCRIPTIVENESS
                severity = RiskLevel.MODERATE
            elif "specimen" in issue_type.lower():
                category = IssueCategory.SPECIMEN_DEFICIENCY
                severity = RiskLevel.LOW
            else:
                category = IssueCategory.PROCEDURAL
                severity = RiskLevel.LOW
            
            # Extract citation
            citation = rag_result.citations_used[0].replace("TMEP ¬ß", "") if rag_result.citations_used else "1207"
            
            # Create issue
            issue = TrademarkIssue(
                category=category,
                severity=severity,
                title=issue_type.title(),
                description=rag_result.analysis[:500],
                tmep_section=citation,
                citation_text=rag_result.retrieved_sections[0].content[:300] if rag_result.retrieved_sections else "",
                recommendation=self._get_recommendation(category, severity),
                confidence=rag_result.confidence,
                estimated_cost=self._estimate_cost(severity),
                estimated_time=self._estimate_time(severity)
            )
            
            issues.append(issue)
        
        return issues
    
    def _get_recommendation(self, category, severity):
        """Get recommendation based on issue type"""
        if category == IssueCategory.LIKELIHOOD_CONFUSION:
            return "Consider consent agreement or amendment to limit goods/services"
        elif category == IssueCategory.DESCRIPTIVENESS:
            return "Gather evidence of acquired distinctiveness or argue suggestiveness"
        else:
            return "Address in Office Action response with supporting evidence"
    
    def _estimate_cost(self, severity) -> str:
        cost_map = {
            RiskLevel.CRITICAL: "$5,000-10,000",
            RiskLevel.HIGH: "$3,000-6,000",
            RiskLevel.MODERATE: "$1,500-3,000",
            RiskLevel.LOW: "$500-1,500",
            RiskLevel.MINIMAL: "$0-500"
        }
        return cost_map.get(severity, "$1,000-2,000")
    
    def _estimate_time(self, severity) -> str:
        time_map = {
            RiskLevel.CRITICAL: "12-18 months",
            RiskLevel.HIGH: "9-12 months",
            RiskLevel.MODERATE: "6-9 months",
            RiskLevel.LOW: "3-6 months",
            RiskLevel.MINIMAL: "1-3 months"
        }
        return time_map.get(severity, "6-9 months")
    
    def _parse_cost(self, cost_str: str) -> int:
        try:
            costs = [int(c.replace('$', '').replace(',', '')) for c in cost_str.split('-')]
            return sum(costs) // len(costs)
        except:
            return 2000
    
    def _parse_time(self, time_str: str) -> int:
        try:
            times = [int(t) for t in time_str.replace('months', '').split('-')]
            return sum(times) // len(times)
        except:
            return 6
    
    def _calculate_total_cost(self, issues) -> str:
        total = sum(self._parse_cost(i.estimated_cost) for i in issues)
        return f"${total:,}-${int(total * 1.5):,}"
    
    def _calculate_total_timeline(self, issues) -> str:
        max_time = max([self._parse_time(i.estimated_time) for i in issues]) if issues else 6
        return f"{max_time}-{max_time + 3} months"
    
    def save_report(self, assessment: dict, output_path: str):
        """Save assessment report as JSON"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(assessment, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Report saved to: {output_path}")

def main():
    """Main execution"""
    
    # Paths
    pdf_path = os.path.join("..", "data", "TEAR_POUR_LIVE_MORE_FULL.pdf")
    output_dir = os.path.join("..", "analysis", "reports")
    os.makedirs(output_dir, exist_ok=True)
    
    # Check if PDF exists
    if not os.path.exists(pdf_path):
        print(f"‚ùå ERROR: PDF not found at: {pdf_path}")
        print()
        print("Expected location: C:\\Users\\Lenovo\\trademark-ai\\data\\TEAR_POUR_LIVE_MORE_FULL.pdf")
        return
    
    # Analyze
    analyzer = TrademarkReportAnalyzer()
    assessment = analyzer.analyze_report(pdf_path)
    
    # Save
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"risk_assessment_{timestamp}.json")
    analyzer.save_report(assessment, output_file)
    
    # Print summary
    print()
    print("üìä ASSESSMENT SUMMARY")
    print("=" * 70)
    print(f"Trademark: {assessment['trademark']}")
    print(f"Overall Risk: {assessment['overall_risk']['level'].upper()} ({assessment['overall_risk']['score']:.1f}/100)")
    print(f"Confidence: {assessment['overall_risk']['confidence']*100:.1f}%")
    print(f"Issues Identified: {len(assessment['issues'])}")
    print(f"Prior Marks: {assessment['prior_marks']['total']}")
    print(f"Estimated Cost: {assessment['recommendations']['estimated_total_cost']}")
    print(f"Estimated Timeline: {assessment['recommendations']['estimated_timeline']}")
    print()
    print(f"üìÑ Full report: {output_file}")
    print()

if __name__ == "__main__":
    main()
